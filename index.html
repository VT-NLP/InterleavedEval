<!DOCTYPE html>
<html>
	<head>
		<title>InterleavedEval</title>
        <link rel="stylesheet" href="./index.css">
        <script src="./index.js"></script>
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
        <link rel="shortcut icon" href="./imgs/vision_flan_logo.jpg"/>
	</head>
	<body>
        <!-- NAVBAR -->
        <nav class="navbar">
            <div class="navbarmenu">
                <a class="visionFlan">VISION-FLAN</a>
            </div>
        </nav>
        
        <div class="major_section">
            <h1>Vision-Flan: Scaling Visual Instruction Tuning</h1>
            <p>Created by Virginia Tech's NLP Lab. Sep 21, 2023</p>
            <hr>
            <br>
            <center>
                <img src="./imgs/vision_flan_logo.jpg" width="400px">
                <i><p>Generated by <a href="https://ideogram.ai/">https://ideogram.ai/</a></p></i>
            </center>
            <br>
            <center>
                <div>
                    <a href="https://arxiv.org/pdf/2402.11690.pdf"><button><i class="fa fa-newspaper-o"></i> Paper</button></a>
                    <a href="./tasks.html"><button><i class="fa fa-book"></i> Tasks</button></a>
                    <a href="#data_samples"><button><i class="fa fa-picture-o"></i> Samples</button></a>
                    <a href="#download"><button><i class="fa fa-database"></i> Download</button></a>
                    <a href="#models"><button><i class="fa fa-gears"></i> Models</button></a>
                    <a href="#acknowledgement"><button><i class="fa fa-arrow-circle-o-right"></i> Acknowledgement</button></a>
                </div>
            </center>
            <br>
            <p class="dataset_description"> We introduce Vision-Flan, the largest human-annotated 
                visual instruction tuning dataset that consists of <b> 200+ </b> diverse vision-language 
                tasks derived from <b> 101 </b> open-source computer vision datasets. Each task is equipped with an expert 
                written instruction and carefully designed templates for the inputs and outputs. The dataset encompasses 
                a wide range of tasks such as image captioning, visual question-answering, and 
                visual understanding. Vision-Flan is built to support various researches and applications in vision-language models, pushing the boundaries of understanding and interaction between these two modalities. Researchers and practitioners can leverage this dataset to advance the state of the art vision-language models
                and develop innovative algorithms in a wide range of domains. We showcase an instance for each task on this <a href="./tasks.html">page</a>.
            </p>
        </div><br><br><br><br>

        <div class="section_title" id="data_samples">
            <h1>Data Samples<hr></h1>
        </div>
        <div class="content-block" id="content-block">
            <p class="data_loading">DATA LOADING . . .</p>
        </div>

        <div class="section_title">
            <p class="dataset_description">Each instance consists of 3 primary elements: Image, Instruction, and Ouput.</p>
            <ul>
                <li><p><b>Image:</b> An image that is used as reference when performing the task specified by the instruction.</p></li>
                <li><p><b>Instruction:</b> A description or prompt of a task that is executed by the vision-language model.</p></li>
                <li><p><b>Output:</b> The expected answer to the instruction given the provided image.</p></li>
            </ul>
            <br><br><br>
        </div>


        <div class="section_title" id="collection_annotation">
            <h1>Data Collection and Annotation<hr></h1>
            <br><br>
            <center><img src="./imgs/pipeline_snip.png" width="100%"><br><i><p>Icon source: <a href="https://flaticon.com/">https://flaticon.com</a></p></i></center>
            <br><br>
            <p class="dataset_description">To ensure the coverage and quality of tasks, we proposed an annotation pipeline as demonstrated in the above figure. First, the authors search on the internet to identify interesting vision-language tasks. Second, the tasks are assigned to the annotators and the annotators write download and preprocessing scripts to prepare the data. Once the dataset is processed into the required format, the authors and annotators start discuss potential tasks that can be derived from the existing annotations. Third, the annotators write instructions and templates for each task and the authors provide feedbacks for revising the instructions. This step can repeat multiple times until the instructions meet the requirement. Forth, the annotators upload the processed datasets and instructions to our database. Finally, the authors double-check the correctness of the instructions, images and outputs. The authors also check the grammar and fluency of the instructions. All the annotators are graduate computer science students who have strong background in machine learning and deep learning.
            </p>
        </div>
        
        <br><br><br><br>
        <div class="section_title" id="download">
            <h1>Download<hr></h1>
            <table>
                <tr>
                    <th>File</th>
                    <th>Size on Disk</th>
                    <th>Sample Size</th>
                </tr>
                <tr>
                    <td><a href="https://huggingface.co/datasets/Vision-Flan/vision-flan_191-task_1k">annotation_191-task_1k.json</a></td>
                    <td>108M</td>
                    <td>186k</td>
                </tr>
                <tr>
                    <td><a href="https://huggingface.co/datasets/Vision-Flan/vision-flan_191-task_1k/tree/main">image_191-task_1k.zip</a></td>
                    <td>37GB</td>
                    <td>186k</td>
                </tr>
                <!-- <tr>
                    <td><a href="">instructions.json</a></td>
                    <td>1TB</td>
                    <td>300k</td>
                </tr> -->
            </table><br><br>
            <p class="dataset_description">We provide the download links to the annotations and images above. In the annotations file, we merged instructions and templates with original tasks' inputs and outputs. To train a model on Vision-Flan, you can simply download the annotations and images. The annotations file consists of 191 tasks and for each task we randomly sampled 1K instances which should be sufficient for the purpose of instruction tuning. By now we can not release all tasks since some datasets are not allowed to be distributed.
            </p>
        </div>
        
        <br><br><br><br>
        <div class="section_title" id="models">
            <h1>Models<hr></h1>
            <table>
                <tr>
                    <th>File</th>
                    <th>Langugae model</th>
                    <th>Turning Strategy</th>
                </tr>
                <tr>
                    <td><a href="https://huggingface.co/Vision-Flan/vision-flan_llava">vision-flan_llava</a></td>
                    <td>Vicuna_v1.3_7B</td>
                    <td>Projection layer, Lora</td>
                </tr>
                <tr>
                    <td><a href="https://huggingface.co/Vision-Flan/vision-flan_blip2_xl">vision-flan_blip2_xl</a></td>
                    <td>Flan_T5_xl</td>
                    <td>Qformer</td>
                </tr>
                <tr>
                    <td><a href="https://huggingface.co/Vision-Flan/vision-flan_blip2_xxl">vision-flan_blip2_xxl</a></td>
                    <td>Flan_T5_xxl</td>
                    <td>Qformer</td>
                </tr>
            </table><br><br>
            <p class="dataset_description">To demonstrate the power of Vision-Flan, we train three vision-language models on 205 tasks in Vision-Flan and use 1K randomly sampled for each task. We finetune the pretrained LLaVa-Vicuna_v1.3_7B for 1 epoch on the mixture of Vision-Flan and LLaVa datasets and name the model vision-flan_llava. We finetune BLIP-2 with Flan_T5_xl as the language model for 1 epochs on Vision-Flan dataset and name the model vision-flan_blip2_xl. 
            We finetune BLIP-2 with Flan_T5_xxl as the language model for 1 epoch on Vision-Flan dataset and name the model vision-flan_blip2_xxl. We provide Huggingface links to the models above.
            </p>
        </div>
        
        <!-- <div class="section_title">
            <h2>Model performance<hr></h2>
            <center><img src="./plot/performance.png" style="width: 700px;"></center>
            <p class="dataset_description">To compare the performance of vision-langugae models trained on human labeled datasets and the GPT-4 generated dataset, we train a LLaVa model on the MultiInstruct dataset which was previously the largest human-labled visual-instruct-tuning dataset consisting of 62 diverse tasks. We also train a LLaVa model on our proposed Vision-Flan dataset. We evaluate the zero-shot perform of multiinstruct_llava. vision-flan_llava and the official llava model on three tasks: scienceQA, scienceQA explanation, and VISIT. As one can observe that vision-flan_llava significantly outperform multiinstruct_llava on all three tasks. However, we can still observe the performance gap between vision-langugae model trained GPT-4 geenrated data and human-labeled data. (We use GPT-4 for evaluating the performance. For SciecneQA we ask GPT-4 if the reference is the same as the prediction. For scienceQA explanation and VISIT, we ask GPT-4 to assign a score between 0-5 for a prediction, given the target and the input prompt. We normalize the scores from 0-5 to 0-100.)
            </p>
        </div> -->
        

        <br><br><br><br>
        <div class="section_title">
            <h1>Citation<hr></h1>
            <p class="dataset_description">If you use Vision-Flan in your research, please cite the following papers.</p>
            <center><div class="bibtex">
                <pre><code>
    @article{xu2024vision,
        title={Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning},
        author={Xu, Zhiyang and Feng, Chao and Shao, Rulin and Ashby, Trevor and Shen, Ying and Jin, Di and Cheng, Yu and Wang, Qifan and Huang, Lifu},
        journal={arXiv preprint arXiv:2402.11690},
        year={2024}
        }
                </code></pre></div><br>
                <div class="bibtex">
                <pre><code>
    @inproceedings{DBLP:conf/acl/XuSH23,
        author = {Zhiyang Xu and Ying Shen and Lifu Huang},
        editor = {Anna Rogers and Jordan L. Boyd{-}Graber and Naoaki Okazaki},
        title = {MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning},
        booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada, July 9-14, 2023},
        pages = {11445--11465},
        publisher = {Association for Computational Linguistics},
        year = {2023},
        url = {https://doi.org/10.18653/v1/2023.acl-long.641},
        doi = {10.18653/v1/2023.acl-long.641},
        timestamp = {Thu, 10 Aug 2023 12:35:59 +0200},
        biburl = {https://dblp.org/rec/conf/acl/XuSH23.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
    }
                </code></pre>
            </div></center>
        </div>
        <br><br><br><br>
        <div class="section_title" id="acknowledgement">
            <h1>Acknowledgement<hr></h1>
            <p class="dataset_description"><span style="color:red;">Vision-Flan dataset is for research purpose only. 
                Please carefully check the licenses of the original datasets before using Vision-Flan.</span> 
                We provide the URLs to the original datasets and their Bibtex on this <a href="./bibtex.html">page</a>.
                The images and tasks may be taken down at any time when requested by the original 
                dataset owners or owners of the referenced images. If you hope to take 
                down any tasks or the images, please contact Zhiyang Xu and Lifu Huang at <span class="email_text">zhiyangx@vt.edu</span> and <span class="email_text">lifuh@cs.vt.edu</span>.
            </p>
            <h3>Annotators<hr></h3>
            <p class="dataset_description">We want to thank Shivansh Mathur, Ujjwal Maheshwari, Jiayue Lin, Alok Mehandale, Fatemeh Sarshartehrani, Nila Masrourisaadat, and Manasa Reddy Kandula for the work that they contributed to annotating Vision-Flan.</p>
        </div>
	</body>
</html>